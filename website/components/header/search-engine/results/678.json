{"type":"doc","filename":"devonfw-guide_ide.wiki_LICENSE.asciidoc.html","anchor":"devonfw-guide_devon4j.wiki_guide-configuration-mapping.asciidoc_generate-configuration-metadata","title":"Generate configuration metadata","breadcrumbs":["devonfw-ide","Support","License"],"text":"Generate configuration metadata\n\nYou should further add this dependency to your module containing the *ConfigProperties:\n\n\n\n\n\n\n\nThis will generate configuration metadata so projects using your code can benefit from autocompletion and getting your JavaDoc as tooltip when editing application.properites what makes this approach very powerful.\nFor further details about this please read A Guide to Spring Boot Configuration Metadata.\n\n\n\n\nAuditing\n\nFor database auditing we use hibernate envers. If you want to use auditing ensure you have the following dependency in your pom.xml:\n\n\nListing 6. spring\n\n\n\n\n\nListing 7. quarkus\n\n\n\n\n\n\n\n\nNote\n\n\nThe following part applies only to spring applications. At this point, the Quarkus extension does not provide any additional configurations. For Quarkus applications, simply use the @Audited annotation to enable auditing for an entity class, as described a few lines below or seen here.\n\n\n\n\n\nMake sure that entity manager also scans the package from the devon4j-jpa[-envers] module in order to work properly. And make sure that correct Repository Factory Bean Class is chosen.\n\n\n\n\n\n\n\nNow let your [Entity]Repository extend from DefaultRevisionedRepository instead of DefaultRepository.\n\n\nThe repository now has a method getRevisionHistoryMetadata(id) and getRevisionHistoryMetadata(id, boolean lazy) available to get a list of revisions for a given entity and a method find(id, revision) to load a specific revision of an entity with the given ID or getLastRevisionHistoryMetadata(id) to load last revision.\nTo enable auditing for a entity simply place the @Audited annotation to your entity and all entity classes it extends from.\n\n\n\n\n\n\n\nWhen auditing is enabled for an entity an additional database table is used to store all changes to the entity table and a corresponding revision number. This table is called &lt;ENTITY_NAME&gt;_AUD per default. Another table called REVINFO is used to store all revisions. Make sure that these tables are available. They can be generated by hibernate with the following property (only for development environments).\n\n\n\n\n\n\n\nAnother possibility is to put them in your database migration scripts like so.\n\n\n\n\n\n\n\n\n\n\nAccess-Control\n\nAccess-Control is a central and important aspect of Security. It consists of two major aspects:\n\n\n\n\ndevonfw-guide_devon4j.wiki_guide-access-control.asciidoc_Authentication (Who tries to access?)\n\n\ndevonfw-guide_devon4j.wiki_guide-access-control.asciidoc_Authorization (Is the one accessing allowed to do what he wants to do?)\n\n\n\n\nAuthentication\n\nDefinition:\n\n\n\n\nAuthentication is the verification that somebody interacting with the system is the actual subject for whom he claims to be.\n\n\n\n\nThe one authenticated is properly called subject or principal. There are two forms of principals you need to distinguish while designing your authentication: human users and autonomous systems. While e.g. a Kerberos/SPNEGO Single-Sign-On makes sense for human users, it is pointless for authenticating autonomous systems.  For simplicity, we use the common term user to refer to any principal even though it may not be a human (e.g. in case of a service call from an external system).\n\n\nTo prove the authenticity, the user provides some secret called credentials. The most simple form of credentials is a password.\n\n\nImplementations\n\n\n\n\nNote\n\n\nPlease never implement your own authentication mechanism or credential store. You have to be aware of implicit demands such as salting and hashing credentials, password life-cycle with recovery, expiry, and renewal including email notification confirmation tokens, central password policies, etc. This is the domain of access managers and identity management systems. In a business context you will typically already find a system for this purpose that you have to integrate (e.g. via LDAP). Otherwise you should consider establishing such a system e.g. using keycloak.\n\n\n\n\n\nWe recommend using JWT when possible. For KISS, also try to avoid combining multiple authentication mechanisms (form based, basic-auth, SAMLv2, OAuth, etc.) within the same application (for different URLs).\n\n\nFor spring, check the Spring Security\n\n\nFor quarkus, check the Quarkus Authentication\n\n\n\n\nAuthorization\n\nDefinition:\n\n\n\n\nAuthorization is the verification that an authenticated user is allowed to perform the operation he intends to invoke.\n\n\n\n\nClarification of terms\n\nFor clarification we also want to give a common understanding of related terms that have no unique definition and consistent usage in the wild.\n\n\nTable 28. Security terms related to authorization\n\n\n\n\n\n\nTerm\nMeaning and comment\n\n\n\n\nPermission\nA permission is an object that allows a principal to perform an operation in the system. This permission can be granted (give) or revoked (taken away). Sometimes people also use the term right what is actually wrong as a right (such as the right to be free) can not be revoked.\n\n\nGroup\nWe use the term group in this context for an object that contains permissions. A group may also contain other groups. Then the group represents the set of all recursively contained permissions.\n\n\nRole\nWe consider a role as a specific form of group that also contains permissions. A role identifies a specific function of a principal. A user can act in a role.\nFor simple scenarios a principal has a single role associated. In more complex situations a principal can have multiple roles but has only one active role at a time that he can choose out of his assigned roles. For KISS it is sometimes sufficient to avoid this by creating multiple accounts for the few users with multiple roles. Otherwise at least avoid switching roles at run-time in clients as this may cause problems with related states. Simply restart the client with the new role as parameter in case the user wants to switch his role.\n\n\nAccess Control\nAny permission, group, role, etc., which declares a control for access management.\n\n\n\n\n\nSuggestions on the access model\n\nFor the access model we give the following suggestions:\n\n\n\n\nEach Access Control (permission, group, role, …​) is uniquely identified by a human readable string.\n\n\nWe create a unique permission for each use-case.\n\n\nWe define groups that combine permissions to typical and useful sets for the users.\n\n\nWe define roles as specific groups as required by our business demands.\n\n\nWe allow to associate users with a list of Access Controls.\n\n\nFor authorization of an implemented use case we determine the required permission. Furthermore, we determine the current user and verify that the required permission is contained in the tree spanned by all his associated Access Controls. If the user does not have the permission we throw a security exception and thus abort the operation and transaction.\n\n\nWe avoid negative permissions, that is a user has no permission by default and only those granted to him explicitly give him additional permission for specific things. Permissions granted can not be reduced by other permissions.\n\n\nTechnically we consider permissions as a secret of the application. Administrators shall not fiddle with individual permissions but grant them via groups. So the access management provides a list of strings identifying the Access Controls of a user. The individual application itself contains these Access Controls in a structured way, whereas each group forms a permission tree.\n\n\n\n\n\nNaming conventions\n\nAs stated above each Access Control is uniquely identified by a human readable string. This string should follow the naming convention:\n\n\n\n\n\n\n\nFor Access Control Permissions the «local-name» again follows the convention:\n\n\n\n\n\n\n\nThe segments are defined by the following table:\n\n\nTable 29. Segments of Access Control Permission ID\n\n\n\n\n\n\n\nSegment\nDescription\nExample\n\n\n\n\n«app-id»\nIs a unique technical but human readable string of the application (or microservice). It shall not contain special characters and especially no dot or whitespace. We recommend to use lower-train-case-ascii-syntax. The identity and access management should be organized on enterprise level rather than application level. Therefore permissions of different apps might easily clash (e.g. two apps might both define a group ReadMasterData but some user shall get this group for only one of these two apps). Using the «app-id». prefix is a simple but powerful namespacing concept that allows you to scale and grow. You may also reserve specific «app-id»s for cross-cutting concerns that do not actually reflect a single app e.g to grant access to a geographic region.\nshop\n\n\n«verb»\nThe action that is to be performed on «object». We use Find for searching and reading data. Save shall be used both for create and update. Only if you really have demands to separate these two you may use Create in addition to Save. Finally, Delete is used for deletions. For non CRUD actions you are free to use additional verbs such as Approve or Reject.\nFind\n\n\n«object»\nThe affected object or entity. Shall be named according to your data-model\nProduct\n\n\n\n\nSo as an example shop.FindProduct will reflect the permission to search and retrieve a Product in the shop application. The group shop.ReadMasterData may combine all permissions to read master-data from the shop. However, also a group shop.Admin may exist for the Admin role of the shop application. Here the «local-name» is Admin that does not follow the «verb»«object» schema.\n\n\n\ndevon4j-security\n\nThe module devon4j-security provides ready-to-use code based on spring-security that makes your life a lot easier.\n\n\n\n\n\nFigure 5. devon4j Security Model\n\n\nThe diagram shows the model of devon4j-security that separates two different aspects:\n\n\n\n\nThe Identity- and Access-Management is provided by according products and typically already available in the enterprise landscape (e.g. an active directory). It provides a hierarchy of primary access control objects (roles and groups) of a user. An administrator can grant and revoke permissions (indirectly) via this way.\n\n\nThe application security defines a hierarchy of secondary access control objects (groups and permissions). This is done by configuration owned by the application (see following section). The \"API\" is defined by the IDs of the primary access control objects that will be referenced from the Identity- and Access-Management.\n\n\n\n\n\nAccess Control Config\n\nIn your application simply extend AccessControlConfig to configure your access control objects as code and reference it from your use-cases. An example config may look like this:\n\n\n\n\n\n\n\n\nConfiguration on Java Method level\n\nIn your use-case you can now reference a permission like this:\n\n\n\n\n\n\n\n\nJEE Standard\n\nRole-based Access Control (RBAC) is commonly used for authorization.\nJSR 250 defines a number of common annotations to secure your application.\n\n\n\n\njavax.annotation.security.PermitAll specifies that no access control is required to invoke the specified method(s).\n\n\njavax.annotation.security.DenyAll specifies that no access controls are allowed to invoke the specified method(s).\n\n\njavax.annotation.security.RolesAllowed specifies that only a list of access controls are allowed to invoke the specified method(s).\n\n\njavax.annotation.security.DeclareRoles defines roles for security checking.\n\n\njavax.annotation.security.RunAs specifies the RunAs role for the given components.\n\n\n\n\n@PermitAll, @Denyall, and @RolesAllowed annotations can be applied to both class and method.\nA method-level annotation will override the behaviour of class-level annotation. Using multiple annotations of those 3 is not valid.\n\n\n\n\n\n\n\nPlease note that when specifying multiple arguments to @RolesAllowed those are combined with OR (and not with AND).\nSo if the user has any of the specified access controls, he will be able to access the method.\n\n\nAs a best practice avoid specifying string literals to @RolesAllowed.\nInstead define a class with all access controls as constants and reference them from there.\nThis class is typically called ApplicationAccessControlConfig in devonfw.\n\n\nIn many complicated cases where @PermitAll @DenyAll @RolesAllowed are insufficient e.g. a method should be accessed by a user in role A and not in role B at the same time, you have to verify the user role directly in the method. You can use SecurityContext class to get further needed information.\n\n\nSpring\n\nSpring Security also supports authorization on method level. To use it, you need to add the spring-security-config dependency. If you use Spring Boot, the dependency spring-boot-starter-security already includes spring-security-config. Then you can configure as follows:\n\n\n\n\nprePostEnabled property enables Spring Security pre/post annotations. @PreAuthorize and @PostAuthorize annotations provide expression-based access control. See more here\n\n\nsecuredEnabled property determines if the @Secured annotation should be enabled. @Secured can be used similarly as @RollesAllowed.\n\n\njsr250Enabled property allows us to use the JSR-250 annotations such as @RolesAllowed.\n\n\n\n\n\n\n\n\n\nA further read about the whole concept of Spring Security Authorization can be found here.\n\n\n\nQuarkus\n\nQuarkus comes with built-in security to allow for RBAC based on the common security annotations @RolesAllowed, @DenyAll, @PermitAll on REST endpoints and CDI beans. Quarkus also provides the io.quarkus.security.Authenticated annotation that will permit any authenticated user to access the resource (equivalent to @RolesAllowed(\"**\")).\n\n\n\n\nData-based Permissions\n\nSee data permissions\n\n\n\nAccess Control Schema (deprecated)\n\nThe access-control-schema.xml approach is deprecated. The documentation can still be found in access control schema.\n\n\n\n\n\n\nData-permissions\n\nIn some projects there are demands for permissions and authorization that is dependent on the processed data. E.g. a user may only be allowed to read or write data for a specific region. This is adding some additional complexity to your authorization. If you can avoid this it is always best to keep things simple. However, in various cases this is a requirement. Therefore the following sections give you guidance and patterns how to solve this properly.\n\n\nStructuring your data\n\nFor all your business objects (entities) that have to be secured regarding to data permissions we recommend that you create a separate interface that provides access to the relevant data required to decide about the permission. Here is a simple example:\n\n\n\n\n\n\n\nNow related business objects (entities) can implement this interface. Often such data-permissions have to be applied to an entire object-hierarchy. For security reasons we recommend that also all child-objects implement this interface. For performance reasons we recommend that the child-objects redundantly store the data-permission properties (such as country in the example above) and this gets simply propagated from the parent, when a child object is created.\n\n\n\nPermissions for processing data\n\nWhen saving or processing objects with a data-permission, we recommend to provide dedicated methods to verify the permission in an abstract base-class such as AbstractUc and simply call this explicitly from your business code. This makes it easy to understand and debug the code. Here is a simple example:\n\n\n\n\n\n\n\nBeware of AOP\n\nFor simple but cross-cutting data-permissions you may also use AOP. This leads to programming aspects that reflectively scan method arguments and magically decide what to do. Be aware that this quickly gets tricky:\n\n\n\n\nWhat if multiple of your method arguments have data-permissions (e.g. implement SecurityDataPermission*)?\n\n\nWhat if the object to authorize is only provided as reference (e.g. Long or IdRef) and only loaded and processed inside the implementation where the AOP aspect does not apply?\n\n\nHow to express advanced data-permissions in annotations?\n\n\n\n\nWhat we have learned is that annotations like @PreAuthorize from spring-security easily lead to the \"programming in string literals\" anti-pattern. We strongly discourage to use this anti-pattern. In such case writing your own verifyPermission methods that you manually call in the right places of your business-logic is much better to understand, debug and maintain.\n\n\n\n\nPermissions for reading data\n\nWhen it comes to restrictions on the data to read it becomes even more tricky. In the context of a user only entities shall be loaded from the database he is permitted to read. This is simple for loading a single entity (e.g. by its ID) as you can load it and then if not permitted throw an exception to secure your code. But what if the user is performing a search query to find many entities? For performance reasons we should only find data the user is permitted to read and filter all the rest already via the database query. But what if this is not a requirement for a single query but needs to be applied cross-cutting to tons of queries? Therefore we have the following pattern that solves your problem:\n\n\nFor each data-permission attribute (or set of such) we create an abstract base entity:\n\n\n\n\n\n\n\nThere are some special hibernate annotations @EntityListeners, @FilterDef, and @Filter used here allowing to apply a filter on the country for any (non-native) query performed by hibernate. The entity listener may look like this:\n\n\n\n\n\n\n\nThis will ensure that hibernate implicitly will call these checks for every such entity when it is read from or written to the database. Further to avoid reading entities from the database the user is not permitted to (and ending up with exceptions), we create an AOP aspect that automatically activates the above declared hibernate filter:\n\n\n\n\n\n\n\nFinally to apply this aspect to all Repositories (can easily be changed to DAOs) implement the following advisor:\n\n\n\n\n\n\n\n\nManaging and granting the data-permissions\n\nFollowing our authorization guide we can simply create a permission for each country. We might simply reserve a prefix (as virtual «app-id») for each data-permission to allow granting data-permissions to end-users across all applications of the IT landscape. In our example we could create access controls country.DE, country.US, country.ES, etc. and assign those to the users. The method permissionChecker.getPermittedCountriesForReading() would then scan for these access controls and only return the 2-letter country code from it.\n\n\n\n\n\nCaution\n\n\nBefore you make your decisions how to design your access controls please clarify the following questions:\n\n\n\n\n\n\n\nDo you need to separate data-permissions independent of the functional permissions? E.g. may it be required to express that a user can read data from the countries ES and PL but is only permitted to modify data from PL? In such case a single assignment of \"country-permissions\" to users is insufficient.\n\n\nDo you want to grant data-permissions individually for each application (higher flexibility and complexity) or for the entire application landscape (simplicity, better maintenance for administrators)? In case of the first approach you would rather have access controls like app1.country.GB and app2.country.GB.\n\n\nDo your data-permissions depend on objects that can be created dynamically inside your application?\n\n\nIf you want to grant data-permissions on other business objects (entities), how do you want to reference them (primary keys, business keys, etc.)? What reference is most stable? Which is most readable?\n\n\n\n\n\n\n\n\n\nJWT\n\nJWT (JSON Web Token) is an open standard (see RFC 7519) for creating JSON based access tokens that assert some number of claims.\nWith an IT landscape divided into multiple smaller apps you want to avoid coupling all those apps or services tightly with your IAM (Identity &amp; Access Management).\nInstead your apps simply expects a JWT as bearer-token in the Authorization HTTP header field.\nAll it needs to do for authentication is validating this JWT.\nThe actual authentication is done centrally by an access system (IAM) that authors those JWTs.\nTherefore we recommend to use strong asymmetric cryptography to sign the JWT when it is authored.\nCreate a keypair per environment and keep the private key as a secret only known to the access system authorizing the JWTs.\nYour apps only need to know the public key in order to validate the JWT.\nAny request without a JWT or with an invalid JWT will be rejected (with status code 401).\n\n\nWhen using spring check the JWT Spring-Starter.\nFor quarkus follow Using JWT RBAC.\n\n\n\n\n\nCross-site request forgery (CSRF)\n\nCSRF is a type of malicious exploit of a web application that allows an attacker to induce users to perform actions that they do not intend to perform.\n\n\n\n\n\n\n\nMore details about csrf can be found at https://owasp.org/www-community/attacks/csrf.\n\n\nSecure devon4j server against CSRF\n\nIn case your devon4j server application is not accessed by browsers or the web-client is using JWT based authentication, you are already safe according to CSRF.\nHowever, if your application is accessed from a browser and you are using form based authentication (with session coockie) or basic authentication, you need to enable CSRF protection.\nThis guide will tell you how to do this.\n\n\nDependency\n\nTo secure your devon4j application against CSRF attacks, you only need to add the following dependency:\n\n\n\n\n\n\n\nStarting with devon4j version 2020.12.001 application template, this is all you need to do.\nHowever, if you have started from an older version or you want to understand more, please read on.\n\n\n\nPluggable web-security\n\nTo enable pluggable security via devon4j security starters you need to apply WebSecurityConfigurer to your BaseWebSecurityConfig (your class extending spring-boot’s WebSecurityConfigurerAdapter) as following:\n\n\n\n\n\n\n\n\nCustom CsrfRequestMatcher\n\nIf you want to customize which HTTP requests will require a CSRF token, you can implement your own CsrfRequestMatcher and provide it to the devon4j CSRF protection via qualified injection as following:\n\n\n\n\n\n\n\nPlease note that the exact name (@Named(\"CsrfRequestMatcher\")) is required here to ensure your custom implementation will be injected properly.\n\n\n\nCsrfRestService\n\nWith the devon4j-starter-security-csrf the CsrfRestService gets integrated into your app.\nIt provides an operation to get the CSRF token via an HTTP GET request.\nThe URL path to retrieve this CSRF token is services/rest/csrf/v1/token.\nAs a result you will get a JSON like the following:\n\n\n\n\n\n\n\nThe token value is a strong random value that will differ for each user session.\nIt has to be send with subsequent HTTP requests (when method is other than GET) in the specified header (X-CSRF-TOKEN).\n\n\n\nHow it works\n\nPutting it all together, a browser client should call the CsrfRestService after successfull login to receive the current CSRF token.\nWith every subsequent HTTP request (other than GET) the client has to send this token in the according HTTP header.\nOtherwise the server will reject the request to prevent CSRF attacks.\nTherefore, an attacker might make your browser perform HTTP requests towards your devon4j application backend via &lt;image&gt; elements, &lt;iframes&gt;, etc.\nYour browser will then still include your session coockie if you are already logged in (e.g. from another tab).\nHowever, in case he wants to trigger DELETE or POST requests trying your browser to make changes in the application (delete or update data, etc.) this will fail without CSRF token.\nThe attacker may make your browser retrieve the CSRF token but he will not be able to retrieve the result and put it into the header of other requests due to the same-origin-policy.\nThis way your application will be secured against CSRF attacks.\n\n\n\n\nConfigure devon4ng client for CSRF\n\nDevon4ng client configuration for CSRF is described here\n\n\n\n\n\n\nAspect Oriented Programming (AOP)\n\nAOP is a powerful feature for cross-cutting concerns. However, if used extensive and for the wrong things an application can get unmaintainable. Therefore we give you the best practices where and how to use AOP properly.\n\n\nAOP Key Principles\n\nWe follow these principles:\n\n\n\n\nWe use spring AOP based on dynamic proxies (and fallback to cglib).\n\n\nWe avoid AspectJ and other mighty and complex AOP frameworks whenever possible\n\n\nWe only use AOP where we consider it as necessary (see below).\n\n\n\n\n\nAOP Usage\n\nWe recommend to use AOP with care but we consider it established for the following cross cutting concerns:\n\n\n\n\nTransaction-Handling\n\n\nAuthorization\n\n\nValidation\n\n\nTrace-Logging (for testing and debugging)\n\n\nException facades for services but only if no other solution is possible (use alternatives such as JAX-RS provider instead).\n\n\n\n\n\nAOP Debugging\n\nWhen using AOP with dynamic proxies the debugging of your code can get nasty. As you can see by the red boxes in the call stack in the debugger there is a lot of magic happening while you often just want to step directly into the implementation skipping all the AOP clutter. When using Eclipse this can easily be archived by enabling step filters. Therefore you have to enable the feature in the Eclipse tool bar (highlighted in read).\n\n\n\n\n\n\n\nIn order to properly make this work you need to ensure that the step filters are properly configured:\n\n\n\n\n\n\n\nEnsure you have at least the following step-filters configured and active:\n\n\n\n\n\n\n\n\n\n\n\nException Handling\n\nException Principles\n\nFor exceptions we follow these principles:\n\n\n\n\nWe only use exceptions for exceptional situations and not for programming control flows, etc. Creating an exception in Java is expensive and hence should not be done for simply testing whether something is present, valid or permitted. In the latter case design your API to return this as a regular result.\n\n\nWe use unchecked exceptions (RuntimeException) [1]\n\n\nWe distinguish internal exceptions and user exceptions:\n\n\n\nInternal exceptions have technical reasons. For unexpected and exotic situations, it is sufficient to throw existing exceptions such as IllegalStateException. For common scenarios a own exception class is reasonable.\n\n\nUser exceptions contain a message explaining the problem for end users. Therefore, we always define our own exception classes with a clear, brief, but detailed message.\n\n\n\n\n\nOur own exceptions derive from an exception base class supporting\n\n\n\nunique ID per instance\n\n\nError code per class\n\n\nmessage templating (see I18N)\n\n\ndistinguish between user exceptions and internal exceptions\n\n\n\n\n\n\n\nAll this is offered by mmm-util-core, which we propose as a solution.\nIf you use the devon4j-rest module, this is already included. For Quarkus applications, you need to add the dependency manually.\n\n\nIf you want to avoid additional dependencies, you can implement your own solution for this by creating an abstract exception class ApplicationBusinessException extending from RuntimeException. For an example of this, see our Quarkus reference application.\n\n\n\nException Example\n\nHere is an exception class from our sample application:\n\n\n\n\n\n\n\nThe message templates are defined in the interface NlsBundleRestaurantRoot as following:\n\n\n\n\n\n\n\n\nHandling Exceptions\n\nFor catching and handling exceptions we follow these rules:\n\n\n\n\nWe do not catch exceptions just to wrap or to re-throw them.\n\n\nIf we catch an exception and throw a new one, we always have to provide the original exception as cause to the constructor of the new exception.\n\n\nAt the entry points of the application (e.g. a service operation) we have to catch and handle all throwables. This is done via the exception-facade-pattern via an explicit facade or aspect. The devon4j-rest module already provides ready-to-use implementations for this such as RestServiceExceptionFacade that you can use in your Spring application. For Quarkus, follow the Quarkus guide on exception handling.\nThe exception facade has to …​\n\n\n\nlog all errors (user errors on info and technical errors on error level)\n\n\nensure that the entire exception is passed to the logger (not only the message) so that the logger can capture the entire stacktrace and the root cause is not lost.\n\n\nconvert the error to a result appropriable for the client and secure for Sensitive Data Exposure. Especially for security exceptions only a generic security error code or message may be revealed but the details shall only be logged but not be exposed to the client. All internal exceptions are converted to a generic error with a message like:\n\n\n\nAn unexpected technical error has occurred. We apologize any inconvenience. Please try again later.\n\n\n\n\n\n\n\n\n\n\n\nCommon Errors\n\nThe following errors may occur in any devon application:\n\n\nTable 30. Common Exceptions\n\n\n\n\n\n\n\nCode\nMessage\nLink\n\n\n\n\nTechnicalError\nAn unexpected error has occurred! We apologize any inconvenience. Please try again later.\nTechnicalErrorUserException.java\n\n\nServiceInvoke\n«original message of the cause»\nServiceInvocationFailedException.java\n\n\n\n\n\n\n\n\nInternationalization\n\nInternationalization (I18N) is about writing code independent from locale-specific information.\nFor I18N of text messages we are suggesting\nmmm native-language-support.\n\n\nIn devonfw we have developed a solution to manage text internationalization. devonfw solution comes into two aspects:\n\n\n\n\nBind locale information to the user.\n\n\nGet the messages in the current user locale.\n\n\n\n\nBinding locale information to the user\n\nWe have defined two different points to bind locale information to user, depending on user is authenticated or not.\n\n\n\n\nUser not authenticated: devonfw intercepts unsecured request and extract locale from it. At first, we try to extract a language parameter from the request and if it is not possible, we extract locale from Àccept-language` header.\n\n\nUser authenticated. During login process, applications developers are responsible to fill language parameter in the UserProfile class. This language parameter could be obtain from DB, LDAP, request, etc. In devonfw sample we get the locale information from database.\n\n\n\n\nThis image shows the entire process:\n\n\n\n\n\n\n\n\nGetting internationalizated messages\n\ndevonfw has a bean that manage i18n message resolution, the ApplicationLocaleResolver. This bean is responsible to get the current user and extract locale information from it and read the correct properties file to get the message.\n\n\nThe i18n properties file must be called ApplicationMessages_la_CO.properties where la=language and CO=country. This is an example of a i18n properties file for English language to translate devonfw sample user roles:\n\n\nApplicationMessages_en_US.properties\n\n\n\n\n\n\n\nYou should define an ApplicationMessages_la_CO.properties file for every language that your application needs.\n\n\nApplicationLocaleResolver bean is injected in AbstractComponentFacade class so you have available this bean in logic layer so you only need to put this code to get an internationalized message:\n\n\n\n\n\n\n\n\n\n\n\nService Client\n\nThis guide is about consuming (calling) services from other applications (micro-services). For providing services, see the Service-Layer Guide. Services can be consumed by the client or the server. As the client is typically not written in Java, you should consult the according guide for your client technology. In case you want to call a service within your Java code, this guide is the right place to get help.\n\n\nMotivation\n\nVarious solutions already exist for calling services, such as RestTemplate from spring or the JAX-RS client API. Furthermore, each and every service framework offers its own API as well. These solutions might be suitable for very small and simple projects (with one or two such invocations). However, with the trend of microservices, the invocation of a service becomes a very common use-case that occurs all over the place. You typically need a solution that is very easy to use but supports flexible configuration, adding headers for authentication, mapping of errors from the server, logging success/errors with duration for performance analysis, support for synchronous and asynchronous invocations, etc. This is exactly what this devon4j service-client solution brings to you.\n\n\n\nUsage\n\nSpring\n\n\nFor Spring, follow the Spring rest-client guide.\n\n\nQuarkus\n\n\nFor Quarkus, we recommend to follow the official Quarkus rest-client guide\n\n\n\n\n\n\nTesting\n\nGeneral best practices\n\nFor testing please follow our general best practices:\n\n\n\n\nTests should have a clear goal that should also be documented.\n\n\nTests have to be classified into different integration levels.\n\n\nTests should follow a clear naming convention.\n\n\nAutomated tests need to properly assert the result of the tested operation(s) in a reliable way. E.g. avoid stuff like assertThat(service.getAllEntities()).hasSize(42) or even worse tests that have no assertion at all.\n\n\nTests need to be independent of each other. Never write test-cases or tests (in Java @Test methods) that depend on another test to be executed before.\n\n\nUse AssertJ to write good readable and maintainable tests that also provide valuable feedback in case a test fails. Do not use legacy JUnit methods like assertEquals anymore!\n\n\nFor easy understanding divide your test in three commented sections:\n\n\n\n//given\n\n\n//when\n\n\n//then\n\n\n\n\n\nPlan your tests and test data management properly before implementing.\n\n\nInstead of having a too strong focus on test coverage better ensure you have covered your critical core functionality properly and review the code including tests.\n\n\nTest code shall NOT be seen as second class code. You shall consider design, architecture and code-style also for your test code but do not over-engineer it.\n\n\nTest automation is good but should be considered in relation to cost per use. Creating full coverage via automated system tests can cause a massive amount of test-code that can turn out as a huge maintenance hell. Always consider all aspects including product life-cycle, criticality of use-cases to test, and variability of the aspect to test (e.g. UI, test-data).\n\n\nUse continuous integration and establish that the entire team wants to have clean builds and running tests.\n\n\nPrefer delegation over inheritance for cross-cutting testing functionality. Good places to put this kind of code can be realized and reused via the JUnit @Rule mechanism.\n\n\n\n\n\nTest Automation Technology Stack\n\nFor test automation we use JUnit. However, we are strictly doing all assertions with AssertJ. For mocking we use Mockito.\nIn order to mock remote connections we use WireMock.\n\n\nFor testing entire components or sub-systems we recommend to use for Spring stack spring-boot-starter-test as lightweight and fast testing infrastructure that is already shipped with devon4j-test. For Quarkus, you can add the necessary extensions manually such as quarkus-junit5, quarkus-junit5-mockito, assertj-core etc.\n\n\nIn case you have to use a full blown JEE application server, we recommend to use arquillian. To get started with arquillian, look here.\n\n\n\nTest Doubles\n\nWe use test doubles as generic term for mocks, stubs, fakes, dummies, or spys to avoid confusion. Here is a short summary from stubs VS mocks:\n\n\n\n\nDummy objects specifying no logic at all. May declare data in a POJO style to be used as boiler plate code to parameter lists or even influence the control flow towards the test’s needs.\n\n\nFake objects actually have working implementations, but usually take some shortcut which makes them not suitable for production (an in memory database is a good example).\n\n\nStubs provide canned answers to calls made during the test, usually not responding at all to anything outside what’s programmed in for the test. Stubs may also record information about calls, such as an email gateway stub that remembers the messages it 'sent', or maybe only how many messages it 'sent'.\n\n\nMocks are objects pre-programmed with expectations, which form a specification of the calls they are expected to receive.\n\n\n\n\nWe try to give some examples, which should make it somehow clearer:\n\n\nStubs\n\nBest Practices for applications:\n\n\n\n\nA good way to replace small to medium large boundary systems, whose impact (e.g. latency) should be ignored during load and performance tests of the application under development.\n\n\nAs stub implementation will rely on state-based verification, there is the threat, that test developers will partially reimplement the state transitions based on the replaced code. This will immediately lead to a black maintenance whole, so better use mocks to assure the certain behavior on interface level.\n\n\nDo NOT use stubs as basis of a large amount of test cases as due to state-based verification of stubs, test developers will enrich the stub implementation to become a large monster with its own hunger after maintenance efforts.\n\n\n\n\n\nMocks\n\nBest Practices for applications:\n\n\n\n\nReplace not-needed dependencies of your system-under-test (SUT) to minimize the application context to start of your component framework.\n\n\nReplace dependencies of your SUT to impact the control flow under test without establishing all the context parameters needed to match the control flow.\n\n\nRemember: Not everything has to be mocked! Especially on lower levels of tests like isolated module tests you can be betrayed into a mocking delusion, where you end up in a hundred lines of code mocking the whole context and five lines executing the test and verifying the mocks behavior. Always keep in mind the benefit-cost ratio, when implementing tests using mocks.\n\n\n\n\n\nWireMock\n\nIf you need to mock remote connections such as HTTP-Servers, WireMock offers easy to use functionality. For a full description see the homepage or the github repository. Wiremock can be used either as a JUnit Rule, in Java outside of JUnit or as a standalone process. The mocked server can be configured to respond to specific requests in a given way via a fluent Java API, JSON files and JSON over HTTP. An example as an integration to JUnit can look as follows.\n\n\n\n\n\n\n\nThis creates a server on a randomly chosen free port on the running machine. You can also specify the port to be used if wanted. Other than that there are several options to further configure the server. This includes HTTPs, proxy settings, file locations, logging and extensions.\n\n\n\n\n\n\n\nThis will stub the URL localhost:port/new/offers to respond with a status 200 message containing a header (Content-Type: application/json) and a body with content given in jsonBodyFile.json if the request matches several conditions.\nIt has to be a GET request to ../new/offers with the two given header properties.\n\n\nNote that by default files are located in src/test/resources/__files/. When using only one WireMock server one can omit the this.mockServer in before the stubFor call (static method).\nYou can also add a fixed delay to the response or processing delay with WireMock.addRequestProcessingDelay(time) in order to test for timeouts.\n\n\nWireMock can also respond with different corrupted messages to simulate faulty behaviour.\n\n\n\n\n\n\n\nA GET request to ../fault returns an OK status header, then garbage, and then closes the connection.\n\n\n\n\nIntegration Levels\n\nThere are many discussions about the right level of integration for test automation. Sometimes it is better to focus on small, isolated modules of the system - whatever a \"module\" may be. In other cases it makes more sense to test integrated groups of modules. Because there is no universal answer to this question, devonfw only defines a common terminology for what could be tested. Each project must make its own decision where to put the focus of test automation. There is no worldwide accepted terminology for the integration levels of testing. In general we consider ISTQB. However, with a technical focus on test automation we want to get more precise.\n\n\nThe following picture shows a simplified view of an application based on the devonfw reference architecture. We define four integration levels that are explained in detail below.\nThe boxes in the picture contain parenthesized numbers. These numbers depict the lowest integration level, a box belongs to. Higher integration levels also contain all boxes of lower integration levels. When writing tests for a given integration level, related boxes with a lower integration level must be replaced by test doubles or drivers.\n\n\n\n\n\n\n\nThe main difference between the integration levels is the amount of infrastructure needed to test them. The more infrastructure you need, the more bugs you will find, but the more instable and the slower your tests will be. So each project has to make a trade-off between pros and contras of including much infrastructure in tests and has to select the integration levels that fit best to the project.\n\n\nConsider, that more infrastructure does not automatically lead to a better bug-detection. There may be bugs in your software that are masked by bugs in the infrastructure. The best way to find those bugs is to test with very few infrastructure.\n\n\nExternal systems do not belong to any of the integration levels defined here. devonfw does not recommend involving real external systems in test automation. This means, they have to be replaced by test doubles in automated tests. An exception may be external systems that are fully under control of the own development team.\n\n\nThe following chapters describe the four integration levels.\n\n\nLevel 1 Module Test\n\nThe goal of a isolated module test is to provide fast feedback to the developer. Consequently, isolated module tests must not have any interaction with the client, the database, the file system, the network, etc.\n\n\nAn isolated module test is testing a single classes or at least a small set of classes in isolation. If such classes depend on other components or external resources, etc. these shall be replaced with a test double.\n\n\n\n\n\n\n\nFor an advanced example see here.\n\n\n\nLevel 2 Component Test\n\nA component test aims to test components or component parts as a unit.\nThese tests can access resources such as a database (e.g. for DAO tests).\nFurther, no remote communication is intended here. Access to external systems shall be replaced by a test double.\n\n\n\n\nFor Spring stack, they are typically run with a (light-weight) infrastructure such as spring-boot-starter-test. A component-test is illustrated in the following example:\n\n\n\n\n\n\nThis test will start the entire spring-context of your app (MySpringBootApp). Within the test spring will inject according spring-beans into all your fields annotated with @Inject. In the test methods you can use these spring-beans and perform your actual tests. This pattern can be used for testing DAOs/Repositories, Use-Cases, or any other spring-bean with its entire configuration including database and transactions.\n\n\n\nFor Quarkus, you can similarly inject the CDI beans and perform tests. An example is shown below:\n\n\n\n\n\n\n\n\n\nWhen you are testing use-cases your authorization will also be in place. Therefore, you have to simulate a logon in advance what is done via the login method in the above Spring example.  The test-infrastructure will automatically do a logout for you after each test method in doTearDown.\n\n\n\nLevel 3 Subsystem Test\n\nA subsystem test runs against the external interfaces (e.g. HTTP service) of the integrated subsystem. Subsystem tests of the client subsystem are described in the devon4ng testing guide. In devon4j the server (JEE application) is the subsystem under test. The tests act as a client (e.g. service consumer) and the server has to be integrated and started in a container.\n\n\n\n\nWith devon4j and Spring you can write a subsystem-test as easy as illustrated in the following example:\n\n\n\n\n\n\nEven though not obvious on the first look this test will start your entire application as a server on a free random port (so that it works in CI with parallel builds for different branches) and tests the invocation of a (REST) service including (un)marshalling of data (e.g. as JSON) and transport via HTTP (all in the invocation of the findCountry method).\n\n\n\n\n\nDo not confuse a subsystem test with a system integration test. A system integration test validates the interaction of several systems where we do not recommend test automation.\n\n\n\nLevel 4 System Test\n\nA system test has the goal to test the system as a whole against its official interfaces such as its UI or batches. The system itself runs as a separate process in a way close to a regular deployment. Only external systems are simulated by test doubles.\n\n\nThe devonfw only gives advice for automated system test (TODO see allure testing framework). In nearly every project there must be manual system tests, too. This manual system tests are out of scope here.\n\n\n\nClassifying Integration-Levels\n\nFor Spring stack, devon4j defines Category-Interfaces that shall be used as JUnit Categories.\nAlso devon4j provides abstract base classes that you may extend in your test-cases if you like.\n\n\ndevon4j further pre-configures the maven build to only run integration levels 1-2 by default (e.g. for fast feedback in continuous integration). It offers the profiles subsystemtest (1-3) and systemtest (1-4). In your nightly build you can simply add -Psystemtest to run all tests.\n\n\n\n\nImplementation\n\nThis section introduces how to implement tests on the different levels with the given devonfw infrastructure and the proposed frameworks.\nFor Spring, see Spring Test Implementation\n\n\n\nRegression testing\n\nWhen it comes to complex output (even binary) that you want to regression test by comparing with an expected result, you sould consider Approval Tests using ApprovalTests.Java.\nIf applied for the right problems, it can be very helpful.\n\n\n\nDeployment Pipeline\n\nA deployment pipeline is a semi-automated process that gets software-changes from version control into production. It contains several validation steps, e.g. automated tests of all integration levels.\nBecause devon4j should fit to different project types - from agile to waterfall - it does not define a standard deployment pipeline. But we recommend to define such a deployment pipeline explicitly for each project and to find the right place in it for each type of test.\n\n\nFor that purpose, it is advisable to have fast running test suite that gives as much confidence as possible without needing too much time and too much infrastructure. This test suite should run in an early stage of your deployment pipeline. Maybe the developer should run it even before he/she checked in the code. Usually lower integration levels are more suitable for this test suite than higher integration levels.\n\n\nNote, that the deployment pipeline always should contain manual validation steps, at least manual acceptance testing. There also may be manual validation steps that have to be executed for special changes only, e.g. usability testing. Management and execution processes of those manual validation steps are currently not in the scope of devonfw.\n\n\n\nTest Coverage\n\nWe are using tools (SonarQube/Jacoco) to measure the coverage of the tests. Please always keep in mind that the only reliable message of a code coverage of X% is that (100-X)% of the code is entirely untested. It does not say anything about the quality of the tests or the software though it often relates to it.\n\n\n\nTest Configuration\n\nThis section covers test configuration in general without focusing on integration levels as in the first chapter.\n\n\n\n\nFor Spring, see Configure Test Specific Beans\n\n\nFor Quarkus, see here\n\n\n\n\nConfigure Test Specific Beans\n\nSometimes it can become handy to provide other or differently configured bean implementations via CDI than those available in production. For example, when creating beans using @Bean-annotated methods they are usually configured within those methods. WebSecurityBeansConfig shows an example of such methods.\n\n\n\n\n\n\n\nAccessControlSchemaProvider allows to programmatically access data defined in some XML file, e.g. access-control-schema.xml. Now, one can imagine that it would be helpful if AccessControlSchemaProvider would point to some other file than the default within a test class. That file could provide content that differs from the default.\nThe question is: how can I change resource path of AccessControlSchemaProviderImpl within a test?\n\n\nOne very helpful solution is to use static inner classes.\nStatic inner classes can contain @Bean -annotated methods, and by placing them in the classes parameter in @SpringBootTest(classes = { /* place class here*/ }) annotation the beans returned by these methods are placed in the application context during test execution. Combining this feature with inheritance allows to override methods defined in other configuration classes as shown in the following listing where TempWebSecurityConfig extends WebSecurityBeansConfig. This relationship allows to override public AccessControlSchemaProvider accessControlSchemaProvider(). Here we are able to configure the instance of type AccessControlSchemaProviderImpl before returning it (and, of course, we could also have used a completely different implementation of the AccessControlSchemaProvider interface). By overriding the method the implementation of the super class is ignored, hence, only the new implementation is called at runtime. Other methods defined in WebSecurityBeansConfig which are not overridden by the subclass are still dispatched to WebSecurityBeansConfig.\n\n\n\n\n\n\n\nThe following chapter of the Spring framework documentation explains issue, but uses a slightly different way to obtain the configuration.\n\n\n\nTest Data\n\nIt is possible to obtain test data in two different ways depending on your test’s integration level.\n\n\n\n\nDebugging Tests\n\nThe following two sections describe two debugging approaches for tests. Tests are either run from within the IDE or from the command line using Maven.\n\n\nDebugging with the IDE\n\nDebugging with the IDE is as easy as always. Even if you want to execute a SubsystemTest which needs a Spring context and a server infrastructure to run properly, you just set your breakpoints and click on Debug As → JUnit Test. The test infrastructure will take care of initializing the necessary infrastructure - if everything is configured properly.\n\n\n\nDebugging with Maven\n\nPlease refer to the following two links to find a guide for debugging tests when running them from Maven.\n\n\n\n\nhttp://maven.apache.org/surefire/maven-surefire-plugin/examples/debugging.html\n\n\nhttps://www.eclipse.org/jetty/documentation/jetty-9/index.html#debugging-with-eclipse\n\n\n\n\nIn essence, you first have to start execute a test using the command line. Maven will halt just before the test execution and wait for your IDE to connect to the process. When receiving a connection the test will start and then pause at any breakpoint set in advance.\nThe first link states that tests are started through the following command:\n\n\n\n\n\n\n\nAlthough this is correct, it will run every test class in your project and - which is time consuming and mostly unnecessary - halt before each of these tests.\nTo counter this problem you can simply execute a single test class through the following command (here we execute the TablemanagementRestServiceTest from the restaurant sample application):\n\n\n\n\n\n\n\nIt is important to notice that you first have to execute the Maven command in the according submodule, e.g. to execute the TablemanagementRestServiceTest you have first to navigate to the core module’s directory.\n\n\n\n\n\n\n\nTransfer-Objects\n\nThe technical data model is defined in form of persistent entities.\nHowever, passing persistent entities via call-by-reference across the entire application will soon cause problems:\n\n\n\n\nChanges to a persistent entity are directly written back to the persistent store when the transaction is committed. When the entity is send across the application also changes tend to take place in multiple places endangering data sovereignty and leading to inconsistency.\n\n\nYou want to send and receive data via services across the network and have to define what section of your data is actually transferred. If you have relations in your technical model you quickly end up loading and transferring way too much data.\n\n\nModifications to your technical data model shall not automatically have impact on your external services causing incompatibilities.\n\n\n\n\nTo prevent such problems transfer-objects are used leading to a call-by-value model and decoupling changes to persistent entities.\n\n\nIn the following sections the different types of transfer-objects are explained.\nYou will find all according naming-conventions in the architecture-mapping\n\n\nTo structure your transfer objects, we recommend the following approaches:\n\n\n\n\nETO and CTO\n\n\nDTO\n\n\n\n\nAlso considering the following transfer objects in specific cases:\n\n\n\nSearchCriteriaTo\n\nFor searching we create or generate a «BusinessObject»SearchCriteriaTo representing a query to find instances of «BusinessObject».\n\nTO\n\nThere are typically transfer-objects for data that is never persistent.\nFor very generic cases these just carry the suffix To.\n\nSTO\n\nWe can potentially create separate service transfer objects (STO) (if possible named «BusinessObject»Sto) to keep the service API stable and independent of the actual data-model.\nHowever, we usually do not need this and want to keep our architecture simple.\nOnly create STOs if you need service versioning and support previous APIs or to provide legacy service technologies that require their own isolated data-model.\nIn such case you also need beanmapping between STOs and ETOs/DTOs what means extra effort and complexity that should be avoided.\n\n\n\n\n\n\nBean Mapping in devon4j-spring\n\nWe have developed a solution that uses a BeanMapper that allows to abstract from the underlying implementation. As mentioned in the general bean mapping guide, we started with Dozer a Java Bean to Java Bean mapper that recursively copies data from one object to another. Now we recommend using Orika. This guide will show an introduction to Orika and Dozer bean-mapper.\n\n\nBean-Mapper Dependency\n\nTo get access to the BeanMapper we have to use either of the below dependency in our POM:\n\n\nListing 8. Orika\n\n\n\n\n\nListing 9. Dozer\n\n\n\n\n\n\nBean-Mapper Configuration\n\nBean-Mapper Configuration using Dozer\n\nThe BeanMapper implementation is based on an existing open-source bean-mapping framework.\nIn case of Dozer the mapping is configured src/main/resources/config/app/common/dozer-mapping.xml.\n\n\nSee the my-thai-star dozer-mapping.xml as an example.\nImportant is that you configure all your custom datatypes as &lt;copy-by-reference&gt; tags and have the mapping from PersistenceEntity (ApplicationPersistenceEntity) to AbstractEto configured properly:\n\n\n\n\n\n\n\n\n\nBean-Mapper Configuration using Orika\n\nOrika with devonfw is configured by default and sets some custom mappings for GenericEntity.java to GenericEntityDto.java. To specify and customize the mappings you can create the class BeansOrikaConfig.java that extends the class BaseOrikaConfig.java from the devon4j.orika package. To register a basic mapping, register a ClassMap for the mapperFactory with your custom mapping. Watch the example below and follow the basic Orika mapping configuration guide and the Orika advanced mapping guide.\n\n\nRegister Mappings:\n\n\n\n\n\n\n\n\nBean-Mapper Usage\n\nThen we can get the BeanMapper via dependency-injection what we typically already provide by an abstract base class (e.g. AbstractUc). Now we can solve our problem very easy:\n\n\n\n\n\n\n\n\n\n\n\nDatatypes\n\n\n\nA datatype is an object representing a value of a specific type with the following aspects:\n\n\n\n\nIt has a technical or business specific semantic.\n\n\nIts JavaDoc explains the meaning and semantic of the value.\n\n\nIt is immutable and therefore stateless (its value assigned at construction time and can not be modified).\n\n\nIt is serializable.\n\n\nIt properly implements #equals(Object) and #hashCode() (two different instances with the same value are equal and have the same hash).\n\n\nIt shall ensure syntactical validation so it is NOT possible to create an instance with an invalid value.\n\n\nIt is responsible for formatting its value to a string representation suitable for sinks such as UI, loggers, etc. Also consider cases like a Datatype representing a password where toString() should return something like \"**\" instead of the actual password to prevent security accidents.\n\n\nIt is responsible for parsing the value from other representations such as a string (as needed).\n\n\nIt shall provide required logical operations on the value to prevent redundancies. Due to the immutable attribute all manipulative operations have to return a new Datatype instance (see e.g. BigDecimal.add(java.math.BigDecimal)).\n\n\nIt should implement Comparable if a natural order is defined.\n\n\n\n\nBased on the Datatype a presentation layer can decide how to view and how to edit the value. Therefore a structured data model should make use of custom datatypes in order to be expressive.\nCommon generic datatypes are String, Boolean, Number and its subclasses, Currency, etc.\nPlease note that both Date and Calendar are mutable and have very confusing APIs. Therefore, use JSR-310 or jodatime instead.\nEven if a datatype is technically nothing but a String or a Number but logically something special it is worth to define it as a dedicated datatype class already for the purpose of having a central javadoc to explain it. On the other side avoid to introduce technical datatypes like String32 for a String with a maximum length of 32 characters as this is not adding value in the sense of a real Datatype.\nIt is suitable and in most cases also recommended to use the class implementing the datatype as API omitting a dedicated interface.\n\n\n\n— mmm project\ndatatype javadoc\n\n\n\nSee mmm datatype javadoc.\n\n\nDatatype Packaging\n\nFor the devonfw we use a common packaging schema.\nThe specifics for datatypes are as following:\n\n\n\n\n\n\n\n\n\nSegment\nValue\nExplanation\n\n\n\n\n&lt;component&gt;\n*\nHere we use the (business) component defining the datatype or general for generic datatypes.\n\n\n&lt;layer&gt;\ncommon\nDatatypes are used across all layers and are not assigned to a dedicated layer.\n\n\n&lt;scope&gt;\napi\nDatatypes are always used directly as API even tough they may contain (simple) implementation logic. Most datatypes are simple wrappers for generic Java types (e.g. String) but make these explicit and might add some validation.\n\n\n\n\n\nTechnical Concerns\n\nMany technologies like Dozer and QueryDSL’s (alias API) are heavily based on reflection. For them to work properly with custom datatypes, the frameworks must be able to instantiate custom datatypes with no-argument constructors. It is therefore recommended to implement a no-argument constructor for each datatype of at least protected visibility.\n\n\n\nDatatypes in Entities\n\nThe usage of custom datatypes in entities is explained in the persistence layer guide.\n\n\n\nDatatypes in Transfer-Objects\n\nXML\n\nFor mapping datatypes with JAXB see XML guide.\n\n\n\nJSON\n\nFor mapping datatypes from and to JSON see JSON custom mapping.\n\n\n\n\n\n\n\nCORS configuration in Spring\n\nDependency\n\nTo enable the CORS support from the server side for your devon4j-Spring application, add the below dependency:\n\n\n\n\n\n\n\n\nConfiguration\n\nAdd the below properties in your application.properties file:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nDescription\nHTTP Header\n\n\n\n\nallowCredentials\nDecides the browser should include any cookies associated with the request (true if cookies should be included).\nAccess-Control-Allow-Credentials\n\n\nallowedOrigins\nList of allowed origins (use * to allow all orgins).\nAccess-Control-Allow-Origin\n\n\nallowedMethods\nList of allowed HTTP request methods (OPTIONS, HEAD, GET, PUT, POST, DELETE, PATCH, etc.).\n-\n\n\nallowedHeaders\nList of allowed headers that can be used during the request (use * to allow all headers requested by the client)\nAccess-Control-Allow-Headers\n\n\npathPattern\nAnt-style pattern for the URL paths where to apply CORS. Use \"/**\" to match all URL paths.\n\n\n\n\n\n\n\n\n\nMicroservices in devonfw\n\nThe Microservices architecture is an approach for application development based on a series of small services grouped under a business domain. Each individual service runs autonomously and communicating with each other through their APIs. That independence between the different services allows to manage (upgrade, fix, deploy, etc.) each one without affecting the rest of the system’s services. In addition to that the microservices architecture allows to scale specific services when facing an increment of the requests, so the applications based on microservices are more flexible and stable, and can be adapted quickly to demand changes.\n\n\nHowever, this new approach, developing apps based on microservices, presents some downsides.\n\n\nLet’s see the main challenges when working with microservices:\n\n\n\n\nHaving the applications divided in different services we will need a component (router) to redirect each request to the related microservice. These redirection rules must implement filters to guarantee a proper functionality.\n\n\nIn order to manage correctly the routing process, the application will also need a catalog with all the microservices and its details: IPs and ports of each of the deployed instances of each microservice, the state of each instance and some other related information. This catalog is called Service Discovery.\n\n\nWith all the information of the Service Discovery the application will need to calculate and select between all the available instances of a microservice which is the suitable one. This will be figured out by the library Client Side Load Balancer.\n\n\nThe different microservices will be likely interconnected with each other, that means that in case of failure of one of the microservices involved in a process, the application must implement a mechanism to avoid the error propagation through the rest of the services and provide an alternative as a process result. To solve this, the pattern Circuit Breaker can be implemented in the calls between microservices.\n\n\nAs we have mentioned, the microservices will exchange calls and information with each other so our applications will need to provide a secured context to avoid not allowed operations or intrusions. In addition, since microservices must be able to operate in an isolated way, it is not recommended to maintain a session. To meet this need without using Spring sessions, a token-based authentication is used that exchanges information using the json web token (JWT) protocol.\n\n\n\n\nIn addition to all of this we will find other issues related to this particular architecture that we will address fitting the requirements of each project.\n\n\n\n\nDistributed data bases: each instance of a microservice should have only one data base.\n\n\nCentralized logs: each instance of a microservice creates a log and a trace that should be centralized to allow an easier way to read all that information.\n\n\nCentralized configuration: each microservice has its own configuration, so our applications should group all those configurations in only one place to ease the configuration management.\n\n\nAutomatized deployments: as we are managing several components (microservices, catalogs, balancers, etc.) the deployment should be automatized to avoid errors and ease this process.\n\n\n\n\nTo address the above, devonfw microservices has an alternative approach Microservices based on Netflix-Tools.\n\n\n\n\n\nCaching\n\nCaching is a technical approach to improve performance. While it may appear easy on the first sight it is an advanced topic. In general, try to use caching only when required for performance reasons. If you come to the point that you need caching first think about:\n\n\n\n\nWhat to cache?\nBe sure about what you want to cache. Is it static data? How often will it change? What will happen if the data changes but due to caching you might receive \"old\" values? Can this be tolerated? For how long? This is not a technical question but a business requirement.\n\n\nWhere to cache?\nWill you cache data on client or server? Where exactly?\n\n\nHow to cache?\nIs a local cache sufficient or do you need a shared cache?\n\n\n\n\nLocal Cache\n\n\n\nShared Cache\n\nDistributed Cache\n\n\n\n\nProducts\n\n\n\nhttp://ehcache.org/\n\n\nhttp://hazelcast.org/\n\n\nhttp://terracotta.org/\n\n\nhttp://memcached.org/\n\n\n\n\n\nCaching of Web-Resources\n\n\n\nhttp://www.mobify.com/blog/beginners-guide-to-http-cache-headers/\n\n\nhttp://en.wikipedia.org/wiki/Web_cache#Cache_control\n\n\nhttp://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Avoiding_caching\n\n\n\n\n\n\n\n\nFeature-Toggles\n\nThe most software developing teams use Feature-Branching to be able to work in parallel and maintain a stable main branch in the VCS. However Feature-Branching might not be the ideal tool in every case because of big merges and isolation between development groups. In many cases, Feature-Toggles can avoid some of these problems, so these should definitely be considered to be used in the collaborative software development.\n\n\nImplementation with the devonfw\n\nTo use Feature-Toggles with the devonfw, use the Framework Togglz because it has all the features generally needed and provides a great documentation.\n\n\nFor a pretty minimal working example, also see this fork.\n\n\nPreparation\n\nThe following example takes place in the oasp-sample-core project, so the necessary dependencies have to be added to the according pom.xml file. Required are the main Togglz project including Spring support, the Togglz console to graphically change the feature state and the Spring security package to handle authentication for the Togglz console.\n\n\n\n\n\n\n\nIn addition to that, the following lines have to be included in the spring configuration file application.properties\n\n\n\n\n\n\n\n\nSmall features\n\nFor small features, a simple query of the toggle state is often enough to achieve the desired functionality. To illustrate this, a simple example follows, which implements a toggle to limit the page size returned by the staffmanagement. See here for further details.\n\n\nThis is the current implementation to toggle the feature:\n\n\n\n\n\n\n\nTo realise this more elegantly with Togglz, first an enum is required to configure the feature-toggle.\n\n\n\n\n\n\n\nTo familiarize the Spring framework with the enum, add the following entry to the application.properties file.\n\n\n\n\n\n\n\nAfter that, the toggle can be used easily by calling the isActive() method of the enum.\n\n\n\n\n\n\n\nThis way, you can easily switch the feature on or off by using the administration console at http://localhost:8081/devon4j-sample-server/togglz-console. If you are getting redirected to the login page, just sign in with any valid user (eg. admin).\n\n\n\nExtensive features\n\nWhen implementing extensive features, you might want to consider using the strategy design pattern to maintain the overview of your software. The following example is an implementation of a feature which adds a 25% discount to all products managed by the offermanagement.\n\n\nTherefore there are two strategies needed:\n\n\nReturn the offers with the normal price\n\n\nReturn the offers with a 25% discount\n\n\n\n\nThe implementation is pretty straight forward so use this as a reference. Compare this for further details.\n\n\n\n\n\n\n\n\n\nGuidelines for a successful use of feature-toggles\n\nThe use of feature-toggles requires a specified set of guidelines to maintain the overview on the software. The following is a collection of considerations and examples for conventions that are reasonable to use.\n\n\nMinimize the number of toggles\n\nWhen using too many toggles at the same time, it is hard to maintain a good overview of the system and things like finding bugs are getting much harder. Additionally, the management of toggles in the configuration interface gets more difficult due to the amount of toggles.\n\n\nTo prevent toggles from piling up during development, a toggle and the associated obsolete source code should be removed after the completion of the corresponding feature. In addition to that, the existing toggles should be revisited periodically to verify that these are still needed and therefore remove legacy toggles.\n\n\n\nConsistent naming scheme\n\nA consistent naming scheme is the key to a structured and easily maintainable set of features. This should include the naming of toggles in the source code and the appropriate naming of commit messages in the VCS. The following section contains an example for a useful naming scheme including a small example.\n\n\nEvery Feature-Toggle in the system has to get its own unique name without repeating any names of features, which were removed from the system. The chosen names should be descriptive names to simplify the association between toggles and their purpose. If the feature should be split into multiple sub-features, you might want to name the feature like the parent feature with a describing addition. If for example you want to split the DISCOUNT feature into the logic and the UI part, you might want to name the sub-features DISCOUNT_LOGIC and DISCOUNT_UI.\n\n\nThe entry in the togglz configuration enum should be named identically to the aforementioned feature name. The explicitness of feature names prevents a confusion between toggles due to using multiple enums.\n\n\nCommit messages are very important for the use of feature-toggles and also should follow a predefined naming scheme. You might want to state the feature name at the beginning of the message, followed by the actual message, describing what the commit changes to the feature. An example commit message could look like the following:\n\n\n\n\n\n\n\nMentioning the feature name in the commit message has the advantage, that you can search your git log for the feature name and get every commit belonging to the feature. An example for this using the tool grep could look like this.\n\n\n\n\n\n\n\nTo keep track of all the features in your software system, a platform like GitHub offers issues. When creating an issue for every feature, you can retrace, who created the feature and who is assigned to completing its development. When referencing the issue from commits, you also have links to all the relevant commits from the issue view.\n\n\n\nPlacement of toggle points\n\nTo maintain a clean codebase, you definitely want to avoid using the same toggle in different places in the software. There should be one single query of the toggle which should be able to toggle the whole functionality of the feature. If one single toggle point is not enough to switch the whole feature on or off, you might want to think about splitting the feature into multiple ones.\n\n\n\nUse of fine-grained features\n\nBigger features in general should be split into multiple sub-features to maintain the overview on the codebase. These sub-features get their own feature-toggle and get implemented independently.\n\n\n\n\n\n\n\nAccessibility\n\nTODO\n\n\nhttp://www.w3.org/TR/WCAG20/\n\n\nhttp://www.w3.org/WAI/intro/aria\n\n\nhttp://www.einfach-fuer-alle.de/artikel/bitv/\n\n\n\n\n\n\n\n\n\n\ndevon4j-kafka has been abandoned. Its main feature was the implementation of a retry pattern using multiple topics. This implementation has become an integral part of Spring Kafka. We recommend to use Spring Kafkas own implemenation for retries.\n\n\n\n\n\n\nMessaging Services\n\nMessaging Services provide an asynchronous communication mechanism between applications. Technically this is implemented using Apache Kafka .\n\n\nFor spring, devonfw uses Spring-Kafka as kafka framework.\nFor more details, check the devon4j-kafka.\n\n\nFor quarkus, follow Quarkus - Kafka Reference Guide.\n\n\n\n\n\nMessaging\n\nMessaging in Java is done using the JMS standard from JEE.\n\n\nProducts\n\nFor messaging you need to choose a JMS provider such as:\n\n\n\n\nRabbitMQ\n\n\nActiveMQ\n\n\nOracle Advanced Queuing (esp. if you already use Oracle RDBMS)\n\n\n\n\n\nReceiver\n\nAs a receiver of messages is receiving data from other systems it is located in the service-layer.\n\n\nJMS Listener\n\nA JmsListener is a class listening and consuming JMS messages. It should carry the suffix JmsListener and implement the MessageListener interface or have its listener method annotated with @JmsListener. This is illustrated by the following example:\n\n\n\n\n\n\n\n\n\nSender\n\nThe sending of JMS messages is considered as any other sending of data like kafka messages or RPC calls via REST using service-client, gRPC, etc.\nThis will typically happen directly from a use-case in the logic-layer.\nHowever, the technical complexity of the communication and protocols itself shall be hidden from the use-case and not be part of the logic layer.\nWith spring we can simply use JmsTemplate to do that.\n\n\n\n\n\n\nFull Text Search\n\nIf you want to all your users fast and simple searches with just a single search field (like in google), you need full text indexing and search support.\n\n\nSolutions\n\n\n\nLucene\n\n\nNGram\n\n\nSolr\n\n\nelastic-search\n\n\n\n\nMaybe you also want to use native features of your database\n\n\n\n\nSAP Hana Fuzzy Search\n\n\nOracle Text\n\n\n\n\n\nBest Practices\n\nTODO\n\n\n\n\nTutorials\n\n\nCreating a new application\n\nRunning the archetype\n\nIn order to create a new application you must use the archetype provided by devon4j which uses the maven archetype functionality.\n\n\nTo create a new application, you should have installed devonfw IDE. Follow the devon ide documentation to install\nthe same.\nYou can choose between 2 alternatives, create it from command line or, in more visual manner, within eclipse.\n\n\nFrom command Line\n\nTo create a new devon4j application from command line, you can simply run the following command:\n\n\n\n\n\n\n\nFor low-level creation you can also manually call this command:\n\n\n\n\n\n\n\nAttention: The archetypeVersion (first argument) should be set to the latest version of devon4j. You can easily determine the version from this badge:\n\n\n\nFurther providing additional properties (using -D parameter) you can customize the generated app:\n\n\nTable 31. Options for app template\n\n\n\n\n\n\n\nproperty\ncomment\nexample\n\n\n\n\ndbType\nChoose the type of RDBMS to use (hana, oracle, mssql, postgresql, mariadb, mysql, etc.)\n-DdbTpye=postgresql\n\n\nbatch\nOption to add an batch module\n-Dbatch=batch\n\n\n\n\n\nFrom Eclipse\n\n\n\n\n\n\n\n\nCreate a new Maven Project.\n\n\nChoose the devon4j-template-server archetype, just like the image.\n\n\n\n\n\n\n\n\n\n\n\nFill the Group Id, Artifact Id, Version and Package for your project.\n\n\n\n\n\n\n\n\n\n\n\nFinish the Eclipse assistant and you are ready to start your project.\n\n\n\n\n\n\nWhat is generated\n\nThe application template (archetype) generates a Maven multi-module project. It has the following modules:\n\n\n\n\napi: module with the API (REST service interfaces, transferobjects, datatypes, etc.) to be imported by other apps as a maven dependency in order to invoke and consume the offered (micro)services.\n\n\ncore: maven module containing the core of the application.\n\n\nbatch: optional module for batch(es)\n\n\nserver: module that bundles the entire app (core with optional batch) as a WAR file.\n\n\n\n\nThe toplevel pom.xml of the generated project has the following features:\n\n\n\n\nProperties definition: Spring-boot version, Java version, etc.\n\n\nModules definition for the modules (described above)\n\n\nDependency management: define versions for dependencies of the technology stack that are recommended and work together in a compatible way.\n\n\nMaven plugins with desired versions and configuration\n\n\nProfiles for test stages\n\n\n\n\n\nHow to run your app\n\nRun app from IDE\n\nTo run your application from your favourite IDE, simply launch SpringBootApp as java application.\n\n\n\nRun app as bootified jar or war\n\nMore details are available here.\n\n\n\n\n\n\n\n\n"}